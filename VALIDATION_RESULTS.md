# ‚ö†Ô∏è VALIDAƒåN√Å ANAL√ùZA NEURAL NETWORK MODELU

**D√°tum:** 2025-10-31
**P√¥vodn√Ω Sharpe:** 2.63
**Z√°ver:** ‚ö†Ô∏è **MODEL JE PRAVDEPODOBNE OVERFITTED**

---

## üìä S√öHRN V√ùSLEDKOV

| Test | Oƒçak√°van√Ω Sharpe | Skutoƒçn√Ω Sharpe | Rozdiel |
|------|------------------|-----------------|---------|
| **Original Test** | - | **2.63** | Baseline |
| **Walk-Forward** | ~2.0 | **0.34** | **-87%** ‚ùå |
| **Cross-Validation** | ~2.0 | **0.39** | **-85%** ‚ùå |
| **Out-of-Sample** | ~2.0 | **0.59** | **-78%** ‚ùå |
| **Monte Carlo** | ~2.0 | **0.82** | **-69%** ‚ùå |
| **Permutation (random)** | ~0 | **1.69** | - ‚ö†Ô∏è |

---

## üîç DETAILN√â V√ùSLEDKY

### 1. WALK-FORWARD VALIDATION (5 folds)

| Fold | Obdobie | Test samples | Sharpe | Correlation |
|------|---------|--------------|--------|-------------|
| 1 | 2021-08 ‚Üí 2022-06 | 210 | **-1.56** | -0.1146 |
| 2 | 2022-06 ‚Üí 2023-04 | 210 | **0.10** | -0.0558 |
| 3 | 2023-04 ‚Üí 2024-02 | 210 | **1.34** | 0.0182 |
| 4 | 2024-02 ‚Üí 2024-12 | 210 | **1.33** | -0.0630 |
| 5 | 2024-12 ‚Üí 2025-10 | 210 | **0.49** | 0.0540 |

**Priemer:** 0.34 ¬± 1.19
**Min:** -1.56
**Max:** 1.34

**Probl√©m:** Veƒæk√° variancia! Sharpe kol√≠≈°e od -1.56 po +1.34. To znamen√°, ≈æe model nie je stabiln√Ω.

---

### 2. TIME-SERIES CROSS-VALIDATION (5 folds)

| Fold | Train | Test | Sharpe | Correlation |
|------|-------|------|--------|-------------|
| 1 | 212 | 210 | **-0.77** | -0.0308 |
| 2 | 422 | 210 | **-0.08** | 0.1501 |
| 3 | 632 | 210 | **1.11** | 0.1110 |
| 4 | 842 | 210 | **0.93** | -0.0851 |
| 5 | 1052 | 210 | **0.74** | -0.0521 |

**Priemer:** 0.39 ¬± 0.79
**Stabilita:** N√≠zka (vysok√° ≈°tandardn√° odch√Ωlka)

---

### 3. OUT-OF-SAMPLE TESTING

**Training:** 2020-2023 (807 samples)

| Rok | Samples | Sharpe | Correlation | Trades |
|-----|---------|--------|-------------|--------|
| 2024 | 252 | **1.06** | -0.0437 | 49 |
| 2025 | 203 | **0.11** | -0.0677 | 43 |

**Priemer:** 0.59

---

### 4. MONTE CARLO SIMUL√ÅCIE (100 runs)

**≈†tatistiky:**
- **Mean Sharpe:** 0.82 ¬± 0.38
- **Median:** 0.82
- **Min:** -0.20
- **Max:** 1.73
- **95% CI:** [-0.11, 1.48]

**Distrib√∫cia:**
- **% Positive:** 95% ‚úÖ
- **% > 1.5:** 3% ‚ùå
- **% > 2.0:** 0% ‚ùå
- **% > 2.63:** 0% ‚ùå

**Z√°ver:** Ani jeden z 100 runov nedosiahol Sharpe > 2.0!

---

### 5. PERMUTATION TESTING (20 runs, random target)

| Metrika | Hodnota |
|---------|---------|
| Mean Sharpe (random) | 0.90 |
| Max Sharpe (random) | **1.69** |
| Original Sharpe | 2.63 |
| P-value | 0.0000 |

**KRITICK√ù PROBL√âM:** Random target dosiahol Sharpe 1.69!

To znamen√°, ≈æe aj s √∫plne n√°hodn√Ωm targetom m√¥≈æeme dosta≈• slu≈°n√© v√Ωsledky. Original Sharpe 2.63 je s√≠ce vy≈°≈°√≠, ale rozdiel nie je tak√Ω v√Ωrazn√Ω ako by mal by≈•.

---

## üö® HLAVN√â PROBL√âMY

### 1. **OVERFITTING NA TEST SETE**
- Original Sharpe 2.63
- Cross-validation priemer: 0.39
- **Rozdiel: 87%!**

‚Üí Model sa nauƒçil ≈°pecifik√° test setu namiesto generalizovateƒæn√Ωch vzorcov.

### 2. **NESTABILITA**
- Walk-forward Sharpe: -1.56 a≈æ +1.34
- ≈†tandardn√° odch√Ωlka: 1.19

‚Üí Model nepredv√≠da konzistentne. V√Ωsledky z√°v isƒæa od obdobia.

### 3. **MAL√ù TESTOVAC√ç SET**
- Test: len 127 samples (~6 mesiacov)
- Train: 883 samples
- Ratio: 14%

‚Üí Pr√≠li≈° mal√Ω test set umo≈æ≈àuje ≈°≈•astn√© trafy.

### 4. **RANDOM BASELINE JE PR√çLI≈† DOBR√ù**
- Random target max Sharpe: 1.69
- Rozdiel oproti original: len 56%

‚Üí Syst√©m m√° pr√≠li≈° veƒæa noise, model zachyt√°va aj n√°hodn√© vzory.

### 5. **≈ΩIADNY RUN > 2.0**
- 100 Monte Carlo simul√°ci√≠
- Max dosiahnut√Ω Sharpe: 1.73
- Original: 2.63

‚Üí Original v√Ωsledok je outlier, nie norma.

---

## üìà POROVNANIE S XGBOOST BASELINE

| Metrika | XGBoost | Neural Net (original) | NN (validated) |
|---------|---------|----------------------|----------------|
| Test Sharpe | 1.34 | 2.63 | **0.82** |
| Cross-Val Sharpe | ? | - | **0.39** |
| Walk-Forward | ? | - | **0.34** |
| Stability | High | Low | Low |

**Z√°ver:** XGBoost baseline (1.34) je pravdepodobne **robustnej≈°√≠** ne≈æ Neural Network!

---

## ‚úÖ ƒåO FUNGUJE

1. **95% Monte Carlo runov je pozit√≠vnych**
   - Model nie je √∫plne zl√Ω
   - Dok√°≈æe vytv√°ra≈• profit

2. **Out-of-sample 2024: Sharpe 1.06**
   - Na niektor√Ωch obdobiach funguje dobre

3. **Permutation test: Max random 1.69 << 2.63**
   - Model sa nauƒçil nieƒço re√°lne (nie len noise)

---

## ‚ùå ƒåO NEFUNGUJE

1. **Veƒæk√° variancia v√Ωsledkov**
   - Sharpe -1.56 a≈æ +1.73
   - Nestabiln√Ω model

2. **Test set bol pr√≠li≈° mal√Ω**
   - 127 samples umo≈æ≈àuje ≈°≈•astie

3. **Overfitting**
   - Train-test gap: 87%

4. **Hyperparameter tuning na test sete**
   - Custom Sharpe loss optimalizuje priamo test metriku
   - To je leaked information!

---

## üéØ REALISTICK√ù OƒåAK√ÅVAN√ù SHARPE

| Scenario | Sharpe | Pravdepodobnos≈• |
|----------|--------|-----------------|
| **Optimistick√Ω** | 1.0-1.5 | 10% |
| **Realistick√Ω** | 0.5-1.0 | 70% |
| **Pesimistick√Ω** | 0-0.5 | 20% |

**Best estimate:** **0.7-0.9** (Monte Carlo priemer)

---

## üí° ODPOR√öƒåANIA

### 1. **POU≈ΩI≈§ XGBOOST NAMIESTO NN**
- XGBoost baseline 1.34 je stabilnej≈°√≠
- Jednoduch≈°√≠ model = men≈°ie riziko overfittingu

### 2. **AK CHCETE POU≈ΩI≈§ NN, TAK:**

#### A) **Zv√§ƒç≈°ite test set**
- Minim√°lne 30% d√°t (nie 10%)
- Walk-forward approach

#### B) **Zme≈àte loss funkciu**
- Nepou≈æ√≠vajte Sharpe loss (leakage!)
- MSE alebo Correlation loss

#### C) **Zjednodu≈°te architekt√∫ru**
- Menej vrstiev (2 namiesto 3)
- Menej parametrov (50k namiesto 91k)
- Viac dropout (0.5 namiesto 0.3)

#### D) **Early stopping je tvoj priateƒæ**
- Zastavil sa na epoch 20 (dobre!)
- Ale validation set bol pr√≠li≈° mal√Ω

#### E) **Ensemble**
- Kombinova≈• NN + XGBoost
- Weighted average: 30% NN + 70% XGBoost

---

## üî¨ TECHNICK√â DETAILY

### Preƒço Monte Carlo < Original?

1. **Random initialization**
   - Ka≈æd√Ω run m√° in√© v√°hy
   - Original mal "≈°≈•astn√©" v√°hy

2. **Early stopping variabilita**
   - Zastav√≠ sa na r√¥znych epoch√°ch
   - Original sa zastavil v optim√°lnom bode

3. **Mal√Ω test set**
   - 127 samples = vysok√° variancia
   - Niektor√© runy maj√∫ smolu, in√© ≈°≈•astie

### Preƒço Permutation baseline je vysok√Ω?

1. **Dataset m√° noise**
   - Ceny maj√∫ momentum a mean reversion
   - Aj random target sa dok√°≈æe "nauƒçi≈•" tieto vzory

2. **Overfitting na train set**
   - Model sa prisp√¥sob√≠ ak√©mukoƒævek targetu
   - 91k parametrov na 883 samples = 100 parametrov/sample!

3. **Features s√∫ korelovan√©**
   - 117 features, ale re√°lna dimensionalita je ni≈æ≈°ia
   - Model n√°jde korel√°cie aj s random targetom

---

## üìä FIN√ÅLNY VERDIKT

### ‚ö†Ô∏è **NEURAL NETWORK MODEL JE OVERFITTED**

**D√¥kazy:**
1. ‚ùå Cross-val Sharpe (0.39) << Test Sharpe (2.63)
2. ‚ùå Walk-forward Sharpe (0.34) << Test Sharpe
3. ‚ùå ≈Ωiadny Monte Carlo run > 2.0
4. ‚ùå Random target dosiahol 1.69
5. ‚ùå Veƒæk√° variancia (-1.56 a≈æ +1.73)

**Realistick√Ω Sharpe:** **0.7-0.9** (nie 2.63)

**Odpor√∫ƒçanie:**
- **Pre produkciu: XGBOOST (Sharpe 1.34)**
- Alebo: Ensemble 30% NN + 70% XGBoost

---

## üìù LESSONS LEARNED

1. **V≈ædy validuj na viacer√Ωch obdobiach**
2. **Mal√Ω test set = vysok√© riziko overfittingu**
3. **Custom loss functions m√¥≈æu leakn√∫≈• inform√°cie**
4. **Monte Carlo je nutnos≈• pre neural networks**
5. **Jednoduch≈°√≠ model > komplexn√Ω model**
6. **XGBoost baseline (1.34) bol v skutoƒçnosti dobr√Ω!**

---

**Pozn√°mka:** Toto je brut√°lne √∫primn√° anal√Ωza. Original Sharpe 2.63 bol pr√≠li≈° pekn√Ω na to, aby bol pravdiv√Ω. Validation to potvrdila.

**Next steps:**
1. Sk√∫si≈• ensemble approach
2. Retrain s v√§ƒç≈°√≠m test setom (30%)
3. Pou≈æi≈• simpler architecture
4. Alebo jednoducho pou≈æi≈• XGBoost üòä
